# ğŸ§  Chat_NLP: A Personalized WhatsApp Chatbot

## ğŸ“Œ Description

This project explores the creation of a neural conversational model trained on real WhatsApp chat history. The goal is to build a chatbot that mimics the communication style of the user **Alejandro** generating context-aware responses based on previous messages using a sequence-to-sequence (seq2seq) architecture with LSTM networks.

## ğŸ“ Dataset

> ğŸ”’ **Note:** The WhatsApp chat dataset used in this project contains personal and private information. For privacy and security reasons, the dataset is **not included** in this repository. Only the preprocessing and training code is provided so users can replicate the pipeline with their own data.


The dataset is a WhatsApp chat export in `.txt` format between two users:
- **Linda**
- **Alejandro** (target speaker)

The chat history was parsed and cleaned to extract valid dialogue turns, excluding media messages and deleted content. From 203,094 messages, over 40,000 valid inputâ€“response pairs were generated for training the model.

## âœ… Whatâ€™s Implemented

- ğŸ“„ **Chat Parsing:** Regex-based extraction of messages, users, and timestamps.
- ğŸ” **Dialogue Turn Construction:** Consecutive messages are grouped to form coherent turns.
- ğŸ”„ **Inputâ€“Target Pairing:** Each message by Alejandro is paired with the previous message by Linda.
- ğŸ§  **Seq2Seq Model:** Encoderâ€“decoder architecture using LSTM layers implemented with TensorFlow/Keras.
- ğŸ§ª **Inference Pipeline:** Real-time decoding loop to generate responses token-by-token.
- ğŸ’¬ **Interactive Chat Interface:** A console-based chatbot (Alejo_Bot) simulates WhatsApp-style conversation.

## ğŸ› ï¸ Tools

- Python
- TensorFlow / Keras
- NumPy, Pandas, re
- Google Colab (for training and experimentation)
- WhatsApp exported `.txt` file

> âš ï¸ **Note:** Due to GPU memory limitations on Google Colab, the model was trained with:
> - `batch_size = 32`  
> - `embedding_dim = 128`  
> - `lstm_units = 256`  
>
> These values were selected to balance training feasibility with performance.

## ğŸ“Š Results

- Model trained for 5 epochs with the limited configuration above.
- Final **validation perplexity**: ~98.76.
- Although accuracy metrics are low (due to the nature of free-form language generation), the model is able to generate plausible, stylistically aligned responses.
- Example:
  ```
  Input: Hola
  Output: cÃ³mo vas?
  ```

## ğŸ”® Future Work

- ğŸ” Train for more epochs and fine-tune hyperparameters (e.g., embedding size, LSTM units).
- ğŸ“ˆ Evaluate using BLEU, ROUGE, and human evaluation metrics for dialogue quality.
- ğŸŒ Expand to multilingual models and multi-speaker chat modeling.
- ğŸ“± Deploy via a frontend interface (e.g., Streamlit, Telegram bot, or WhatsApp bot via Twilio).
- ğŸ¤— Explore transfer learning with pre-trained LLMs (e.g., T5, BART, GPT) for improved generation.

## ğŸ§¾ Conclusion

This project demonstrates the feasibility of training a neural chatbot from personal chat history using classical seq2seq architecture. It showcases end-to-end pipeline development â€” from data wrangling and preprocessing to model training and interactive inference â€” making it a strong baseline for personalized chatbot applications. While the current implementation is basic, it provides a solid foundation for further experimentation and real-world deployment.
